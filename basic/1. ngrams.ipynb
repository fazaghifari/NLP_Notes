{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007b6497-1981-41c6-a6c1-61c4aaff7d6b",
   "metadata": {},
   "source": [
    "# Probabilistic Language Modeling\n",
    "\n",
    "**Goal:**\n",
    "- To compute the probability of a sentence (i.e. sequence of words)\n",
    "$$\n",
    "P(W)= P(w_1,w_2,w_3,\\ldots,w_n)\n",
    "$$\n",
    "- Example:\n",
    "    - Probability of \"High winds tonight\" > Probability of \"Large winds tonight\"\n",
    "    - P(\"The post office is 15 minutes from here\") > P(\"The post office is 15 minuets from here\")\n",
    "    - P(\"I want to eat steak\") > P(\"I want to ride steak\")\n",
    "\n",
    "**Related Task Example:**\n",
    "- Calculate the probability of upcoming word:\n",
    "$$\n",
    "P(w_4|w_1,w_2,w_3)\n",
    "$$\n",
    "- Example:\n",
    "    - In the context of text in restaurant ads:\n",
    "    - $P(\\mathrm{eat}|\\mathrm{I}, \\mathrm{want}, \\mathrm{to}) > P(P(\\mathrm{sleep}|\\mathrm{I}, \\mathrm{want}, \\mathrm{to}))$\n",
    "\n",
    "**This is called the Basic Language Model**\n",
    "- Grammar model is better.\n",
    "- But sometimes, this model is sufficient.\n",
    "- And, it's relatively easy to compute compared to grammar model.\n",
    "\n",
    "---\n",
    "## Calculating The Probability\n",
    "\n",
    "$$\n",
    "P(\\mathrm{I}, \\mathrm{want}, \\mathrm{to},\\mathrm{eat})\n",
    "$$\n",
    "\n",
    "Use the chain rule!\n",
    "\n",
    "$$\n",
    "P(a,b,c,d) = P(a)\\cdot P(b|a) \\cdot P(c|a,b) \\cdot P(d|a,b,c)\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "P(x_1,x_2,\\ldots,x_n) = P(x_1)\\cdot P(x_2|x_1) \\cdot \\ldots \\cdot P(x_n|x_1,\\ldots,x_{n-1})\n",
    "$$\n",
    "\n",
    "**Problem**\n",
    "\n",
    "Estimating the probability:\n",
    "\n",
    "$$\n",
    "P(\\mathrm{eat}|\\mathrm{I}, \\mathrm{want}, \\mathrm{to}) = \\frac{Count(I, want, to, eat)}{Count(I, want, to)}\n",
    "$$\n",
    "\n",
    "* Too many possibilites!\n",
    "* Hard to estimate!\n",
    "\n",
    "**Simplification**\n",
    "\n",
    "Using Markov assumption:\n",
    "- $P(\\mathrm{eat}|\\mathrm{I}, \\mathrm{want}, \\mathrm{to}) \\approx P(\\mathrm{eat}|\\mathrm{to})$\n",
    "- $P(\\mathrm{eat}|\\mathrm{I}, \\mathrm{want}, \\mathrm{to}) \\approx P(\\mathrm{eat}|\\mathrm{want}, \\mathrm{to})$\n",
    "---\n",
    "### Unigram Model\n",
    "\n",
    "$$\n",
    "P(w_1,w_2,\\ldots,w_i) \\approx \\prod_i P(w_i)\n",
    "$$\n",
    "\n",
    "- Only see the probability of each word.\n",
    "- Do not see context / previous words.\n",
    "- Simplest model.\n",
    "- But bad.\n",
    "- Just no, unless there's no other way.\n",
    "\n",
    "---\n",
    "### Bigram Model\n",
    "\n",
    "$$\n",
    "P(w_i|w_1,w_2,\\ldots,w_{i-1}) \\approx \\prod_i P(w_i|w_{i-1})\n",
    "$$\n",
    "\n",
    "- Better than unigram.\n",
    "- See the context up to 1 word before.\n",
    "- Not very complex.\n",
    "- Sometimes, good enough\n",
    "\n",
    "**Estimating bigram**\n",
    "\n",
    "$$\n",
    "P(w_i|w_{i-1}) = \\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\n",
    "$$\n",
    "\n",
    "---\n",
    "### Tri, 4, 5, etc grams\n",
    "- Have longer conditional.\n",
    "- Usually better\n",
    "- But the probability estimation is harder to calculate.\n",
    "    - i.e. the probability of longer sentence is smaller.\n",
    "---\n",
    "### Practical Tips\n",
    "- Do everything in log space\n",
    "    - Avoid underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03dab5a9-cc27-447c-8ac0-6e6ab961442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability : 3.0000000000000004e-07\n",
      "log-probability : -15.01948336229021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p1 = 0.1\n",
    "p2 = 0.02\n",
    "p3 = 0.015\n",
    "p4 = 0.2\n",
    "p5 = 0.05\n",
    "\n",
    "p = p1*p2*p3*p4*p5\n",
    "logp = np.log(p1) + np.log(p2) + np.log(p3) + np.log(p4) + np.log(p5)\n",
    "\n",
    "print(f'probability : {p}')\n",
    "print(f'log-probability : {logp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29585ad-a042-43e9-9755-3d2d8bcd674e",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "How to evaluate the language model?\n",
    "\n",
    "Using **Perplexity**:\n",
    "- Measurement of how well probability model predicts a sample.\n",
    "- Low perplexity indicates the model is good at predicting sample.\n",
    "- Minimizing Perplexity is equal to maximizing probability\n",
    "\n",
    "<img src=\"https://images1.programmersought.com/109/ed/ed7002017e9dcf5bfdaed1a0dc845d55.png\" width= 500px;/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288df976-c514-4249-b99a-a1f7e485c91a",
   "metadata": {},
   "source": [
    "---\n",
    "## Unknown Words\n",
    "- Words that we have never seen before.\n",
    "- So far, our vocabulary is closed. (Only contains some set of words)\n",
    "- What if our model face words that have never seen before?\n",
    "- Use OOV token:\n",
    "    - Percentage of OOV words that appear in a test called OOV rate.\n",
    "    - We usually use a pseudo-word `<UNK>` to assign this token.\n",
    "- Two common way to use `<UNK>`:\n",
    "- First:\n",
    "    - Choose a vocabulary in advance.\n",
    "    - Convert in the training set, any word that not in the vocab as `<UNK>`\n",
    "    - Estimate the probability of `<UNK>` from its counts just like any other regular word.\n",
    "- Second:\n",
    "    - Similar with the first approach.\n",
    "    - But instead, we choose the vocabulary based on frequency.\n",
    "    - e.g: Words that occur less than $n$ times assigned as `<UNK>`\n",
    "\n",
    "---\n",
    "## Smoothing\n",
    "\n",
    "- If there are words in our vocab but appear in a test set in an unseen context:\n",
    "- Example:\n",
    "    - Training set: \"I want to learn english language\"\n",
    "    - Test set: \"I want to eat english breakfast\"\n",
    "    - Word \"English\" never comes after word \"eat\"\n",
    "    - The probability would be zero.\n",
    "- Use **Smoothing** technique:\n",
    "    - Add-one smoothing (Laplace)\n",
    "    - Add-k\n",
    "    - Stupid Backoff\n",
    "    - Kneser-Ney (Most recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e80f4-610e-4edf-8b75-2c916ad4d591",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853749e4-a1f1-4d03-bbf4-364a11155566",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a58c8b-ccf8-4480-8972-ae468714a13e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "155cecb8-764e-46df-8cb2-242697f7fe4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
